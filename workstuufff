import pandas as pd
import numpy as np

def analyze_data_coverage(accts, df_all):
    """
    Analyze the coverage between account data and detailed data
    """
    print("="*70)
    print("DATA COVERAGE ANALYSIS")
    print("="*70)
    
    # Get unique combinations in each dataset
    accts_segments = accts[['cntry_cd', 'bus_entity', 'prd_cd', 'pd_seg', 'bhvr_score_aligned']].drop_duplicates()
    df_all_segments = df_all[['cntry_cd', 'bus_entity', 'prd_cd', 'pd_seg', 'bhvr_score_aligned']].drop_duplicates()
    
    print(f"Unique segment combinations:")
    print(f"- accts: {len(accts_segments)}")
    print(f"- df_all: {len(df_all_segments)}")
    
    # Find segments in df_all but not in accts
    merged_check = df_all_segments.merge(
        accts_segments, 
        on=['cntry_cd', 'bus_entity', 'prd_cd', 'pd_seg', 'bhvr_score_aligned'],
        how='left',
        indicator=True
    )
    df_all_segments['in_accts'] = merged_check['_merge'] == 'both'
    
    # Handle NA values - treat NA as False (not in accts)
    df_all_segments['in_accts'] = df_all_segments['in_accts'].fillna(False)
    missing_segments = df_all_segments[~df_all_segments['in_accts']]
    print(f"- Segments in df_all but NOT in accts: {len(missing_segments)}")
    
    if len(missing_segments) > 0:
        print("\nSample missing segments:")
        print(missing_segments[['cntry_cd', 'bus_entity', 'prd_cd', 'pd_seg', 'bhvr_score_aligned']].head())
    
    # Coverage by country-product
    coverage_by_cp = []
    for (country, product), group in df_all_segments.groupby(['cntry_cd', 'prd_cd']):
        total_segments = len(group)
        covered_segments = group['in_accts'].sum()
        coverage_pct = (covered_segments / total_segments) * 100
        coverage_by_cp.append({
            'cntry_cd': country,
            'prd_cd': product,
            'total_segments': total_segments,
            'covered_segments': covered_segments,
            'coverage_pct': coverage_pct
        })
    
    coverage_df = pd.DataFrame(coverage_by_cp)
    print(f"\nCoverage by Country-Product:")
    print(coverage_df)
    
    return missing_segments, coverage_df

def create_hierarchical_weights(accts, exposure_col='os_bal_cad_amt'):
    """
    Create hierarchical weights to handle missing segments
    Strategy: Create weights at multiple levels to fill gaps
    """
    print("\n" + "="*70)
    print("CREATING HIERARCHICAL WEIGHTS")
    print("="*70)
    
    # Level 1: Full segment weights (most granular)
    print("Level 1: Full segment weights...")
    full_segment_weights = accts.groupby(['cntry_cd', 'prd_cd', 'bus_entity', 'pd_seg', 'bhvr_score_aligned']).agg({
        exposure_col: 'sum',
        'acct_num': 'count'
    }).reset_index()
    full_segment_weights = full_segment_weights.rename(columns={exposure_col: 'segment_exposure', 'acct_num': 'account_count'})
    
    # Calculate total exposure by country-product for normalization
    cp_totals = full_segment_weights.groupby(['cntry_cd', 'prd_cd'])['segment_exposure'].sum().reset_index()
    cp_totals = cp_totals.rename(columns={'segment_exposure': 'cp_total_exposure'})
    
    full_segment_weights = full_segment_weights.merge(cp_totals, on=['cntry_cd', 'prd_cd'])
    full_segment_weights['weight_level1'] = full_segment_weights['segment_exposure'] / full_segment_weights['cp_total_exposure']
    full_segment_weights['weight_source'] = 'full_segment'
    
    print(f"Full segment weights: {len(full_segment_weights)}")
    
    # Level 2: Aggregate some dimensions if needed (fallback weights)
    print("Level 2: Creating fallback weights...")
    
    # Option 2a: Aggregate by dropping bhvr_score_aligned
    fallback_2a = accts.groupby(['cntry_cd', 'prd_cd', 'bus_entity', 'pd_seg']).agg({
        exposure_col: 'sum',
        'acct_num': 'count'
    }).reset_index()
    fallback_2a = fallback_2a.rename(columns={exposure_col: 'segment_exposure', 'acct_num': 'account_count'})
    fallback_2a = fallback_2a.merge(cp_totals, on=['cntry_cd', 'prd_cd'])
    fallback_2a['weight_level2a'] = fallback_2a['segment_exposure'] / fallback_2a['cp_total_exposure']
    fallback_2a['weight_source'] = 'no_bhvr_score'
    
    # Option 2b: Aggregate by dropping pd_seg
    fallback_2b = accts.groupby(['cntry_cd', 'prd_cd', 'bus_entity', 'bhvr_score_aligned']).agg({
        exposure_col: 'sum',
        'acct_num': 'count'
    }).reset_index()
    fallback_2b = fallback_2b.rename(columns={exposure_col: 'segment_exposure', 'acct_num': 'account_count'})
    fallback_2b = fallback_2b.merge(cp_totals, on=['cntry_cd', 'prd_cd'])
    fallback_2b['weight_level2b'] = fallback_2b['segment_exposure'] / fallback_2b['cp_total_exposure']
    fallback_2b['weight_source'] = 'no_pd_seg'
    
    # Level 3: Even more aggregated (bus_entity only)
    print("Level 3: High-level fallback weights...")
    fallback_3 = accts.groupby(['cntry_cd', 'prd_cd', 'bus_entity']).agg({
        exposure_col: 'sum',
        'acct_num': 'count'
    }).reset_index()
    fallback_3 = fallback_3.rename(columns={exposure_col: 'segment_exposure', 'acct_num': 'account_count'})
    fallback_3 = fallback_3.merge(cp_totals, on=['cntry_cd', 'prd_cd'])
    fallback_3['weight_level3'] = fallback_3['segment_exposure'] / fallback_3['cp_total_exposure']
    fallback_3['weight_source'] = 'bus_entity_only'
    
    print(f"Fallback 2a weights (no bhvr_score): {len(fallback_2a)}")
    print(f"Fallback 2b weights (no pd_seg): {len(fallback_2b)}")
    print(f"Fallback 3 weights (bus_entity only): {len(fallback_3)}")
    
    return {
        'level1': full_segment_weights,
        'level2a': fallback_2a,
        'level2b': fallback_2b,
        'level3': fallback_3,
        'cp_totals': cp_totals
    }

def assign_weights_with_fallback(df_all, weight_hierarchy):
    """
    Assign weights to df_all using hierarchical fallback strategy
    """
    print("\n" + "="*70)
    print("ASSIGNING WEIGHTS WITH FALLBACK STRATEGY")
    print("="*70)
    
    df_weighted = df_all.copy()
    df_weighted['weight'] = np.nan
    df_weighted['weight_source'] = 'unmatched'
    
    # Try Level 1 first (full segment match)
    print("Attempting Level 1 matches (full segment)...")
    level1_match = df_weighted.merge(
        weight_hierarchy['level1'][['cntry_cd', 'prd_cd', 'bus_entity', 'pd_seg', 'bhvr_score_aligned', 'weight_level1']],
        on=['cntry_cd', 'prd_cd', 'bus_entity', 'pd_seg', 'bhvr_score_aligned'],
        how='left'
    )
    
    mask_l1 = level1_match['weight_level1'].notna()
    df_weighted.loc[mask_l1, 'weight'] = level1_match.loc[mask_l1, 'weight_level1']
    df_weighted.loc[mask_l1, 'weight_source'] = 'full_segment'
    
    print(f"Level 1 matches: {mask_l1.sum()} records")
    
    # Try Level 2a for unmatched (drop bhvr_score_aligned)
    unmatched_mask = df_weighted['weight'].isna()
    if unmatched_mask.sum() > 0:
        print(f"Attempting Level 2a matches (no bhvr_score) for {unmatched_mask.sum()} records...")
        level2a_match = df_weighted[unmatched_mask].merge(
            weight_hierarchy['level2a'][['cntry_cd', 'prd_cd', 'bus_entity', 'pd_seg', 'weight_level2a']],
            on=['cntry_cd', 'prd_cd', 'bus_entity', 'pd_seg'],
            how='left'
        )
        
        mask_l2a = level2a_match['weight_level2a'].notna()
        if mask_l2a.sum() > 0:
            unmatched_indices = df_weighted[unmatched_mask].index[mask_l2a]
            df_weighted.loc[unmatched_indices, 'weight'] = level2a_match.loc[mask_l2a, 'weight_level2a'].values
            df_weighted.loc[unmatched_indices, 'weight_source'] = 'no_bhvr_score'
            print(f"Level 2a matches: {mask_l2a.sum()} records")
    
    # Try Level 2b for still unmatched (drop pd_seg)
    unmatched_mask = df_weighted['weight'].isna()
    if unmatched_mask.sum() > 0:
        print(f"Attempting Level 2b matches (no pd_seg) for {unmatched_mask.sum()} records...")
        level2b_match = df_weighted[unmatched_mask].merge(
            weight_hierarchy['level2b'][['cntry_cd', 'prd_cd', 'bus_entity', 'bhvr_score_aligned', 'weight_level2b']],
            on=['cntry_cd', 'prd_cd', 'bus_entity', 'bhvr_score_aligned'],
            how='left'
        )
        
        mask_l2b = level2b_match['weight_level2b'].notna()
        if mask_l2b.sum() > 0:
            unmatched_indices = df_weighted[unmatched_mask].index[mask_l2b]
            df_weighted.loc[unmatched_indices, 'weight'] = level2b_match.loc[mask_l2b, 'weight_level2b'].values
            df_weighted.loc[unmatched_indices, 'weight_source'] = 'no_pd_seg'
            print(f"Level 2b matches: {mask_l2b.sum()} records")
    
    # Try Level 3 for still unmatched (bus_entity only)
    unmatched_mask = df_weighted['weight'].isna()
    if unmatched_mask.sum() > 0:
        print(f"Attempting Level 3 matches (bus_entity only) for {unmatched_mask.sum()} records...")
        level3_match = df_weighted[unmatched_mask].merge(
            weight_hierarchy['level3'][['cntry_cd', 'prd_cd', 'bus_entity', 'weight_level3']],
            on=['cntry_cd', 'prd_cd', 'bus_entity'],
            how='left'
        )
        
        mask_l3 = level3_match['weight_level3'].notna()
        if mask_l3.sum() > 0:
            unmatched_indices = df_weighted[unmatched_mask].index[mask_l3]
            df_weighted.loc[unmatched_indices, 'weight'] = level3_match.loc[mask_l3, 'weight_level3'].values
            df_weighted.loc[unmatched_indices, 'weight_source'] = 'bus_entity_only'
            print(f"Level 3 matches: {mask_l3.sum()} records")
    
    # Final summary
    final_unmatched = df_weighted['weight'].isna().sum()
    print(f"\nFinal matching summary:")
    print(df_weighted['weight_source'].value_counts())
    print(f"Still unmatched: {final_unmatched} records")
    
    if final_unmatched > 0:
        print("\nWarning: Some records still don't have weights. Options:")
        print("1. Exclude them from analysis")
        print("2. Assign equal weights within country-product")
        print("3. Use overall average weight")
        
        # Option 2: Assign equal weights within country-product for unmatched
        unmatched_mask = df_weighted['weight'].isna()
        if unmatched_mask.sum() > 0:
            print("Applying equal weight strategy for unmatched records...")
            for (country, product), group in df_weighted[unmatched_mask].groupby(['cntry_cd', 'prd_cd']):
                # Find how many unmatched segments in this country-product
                unmatched_in_cp = group.index
                equal_weight = 1.0 / len(unmatched_in_cp)  # Equal distribution
                df_weighted.loc[unmatched_in_cp, 'weight'] = equal_weight
                df_weighted.loc[unmatched_in_cp, 'weight_source'] = 'equal_weight_fallback'
    
    return df_weighted

def aggregate_weighted_results(df_weighted):
    """
    Aggregate to country-product-date level with weight adjustments
    """
    print("\n" + "="*70)
    print("AGGREGATING WEIGHTED RESULTS")
    print("="*70)
    
    # Calculate weighted fli_perf
    df_weighted['weighted_fli_perf'] = df_weighted['fli_perf'] * df_weighted['weight']
    
    # Aggregate to country-product-date level
    final_agg = df_weighted.groupby(['cntry_cd', 'prd_cd', 'cal_dt']).agg({
        'weighted_fli_perf': 'sum',
        'fli_perf': ['mean', 'count', 'std'],
        'weight': 'sum',
        'pd': 'mean',
        'lgd': 'mean'
    }).reset_index()
    
    # Flatten columns
    final_agg.columns = [
        'cntry_cd', 'prd_cd', 'cal_dt', 
        'weighted_fli_perf_sum', 
        'fli_perf_mean', 'record_count', 'fli_perf_std',
        'total_weight', 'avg_pd', 'avg_lgd'
    ]
    
    # Calculate normalized weighted average
    final_agg['weighted_fli_perf_normalized'] = final_agg['weighted_fli_perf_sum'] / final_agg['total_weight']
    
    # Weight coverage analysis
    print("Weight coverage by country-product:")
    coverage_analysis = df_weighted.groupby(['cntry_cd', 'prd_cd'])['weight_source'].value_counts().unstack(fill_value=0)
    print(coverage_analysis)
    
    return final_agg

def main_hierarchical_weighting(accts, df_all, exposure_col='os_bal_cad_amt'):
    """
    Main function with hierarchical weighting to handle missing segments
    """
    print("="*70)
    print("HIERARCHICAL WEIGHTING FOR INCOMPLETE COVERAGE")
    print("="*70)
    
    # Step 1: Analyze coverage
    missing_segments, coverage_df = analyze_data_coverage(accts, df_all)
    
    # Step 2: Create hierarchical weights
    weight_hierarchy = create_hierarchical_weights(accts, exposure_col)
    
    # Step 3: Apply weights with fallback
    df_weighted = assign_weights_with_fallback(df_all, weight_hierarchy)
    
    # Step 4: Aggregate results
    final_results = aggregate_weighted_results(df_weighted)
    
    print(f"\n" + "="*70)
    print("FINAL RESULTS SUMMARY")
    print("="*70)
    print(f"Final aggregated data shape: {final_results.shape}")
    print("\nSample results:")
    print(final_results[['cntry_cd', 'prd_cd', 'cal_dt', 'weighted_fli_perf_normalized', 'total_weight', 'record_count']].head())
    
    return final_results, df_weighted, weight_hierarchy, coverage_df

# Usage:
"""
# Run the hierarchical weighting process
final_results, df_weighted, weights_hierarchy, coverage_analysis = main_hierarchical_weighting(accts, df_all)

# Your weighted fli_perf is in: final_results['weighted_fli_perf_normalized']
# This handles the case where accts has fewer unique pairs than df_all
"""
